{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiopdl/sPCI_sLZc/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#READ ME\n",
        "\n",
        "Not necessarily intended for public use. Please contact jponcedeleon@ucmerced.edu with any questions or comments.\n",
        "\n",
        "1.   Create the following directories in the root of your Google Drive (replacing \"Example\" with any desired name of your project):\n",
        "*  /Example/Data/Inputs/Activations/\n",
        "*  /Example/Data/Inputs/Events/\n",
        "*  /Example/Data/Outputs/Timepoints/\n",
        "*  /Example/Data/Outputs/Measures/\n",
        "*  /Example/Data/Outputs/Measures/Filtered/\n",
        "\n",
        "2.   Put your 2x2 activation matrix CSVs (channels x timepoints, 1 file per subject) in /Example/Data/Inputs/Activations/.\n",
        "3.   Put your corresponding EventList TXTs (1 per subject) in /Example/Data/Inputs/Events/.\n",
        "4.   Run each code chunk from top to bottom (PCI code, Python imports, and helper functions) until you get to Main.\n",
        "5.  In the first code chunk in Main, configure the parameters accordingly.\n",
        "6.   Run the second code in chunk Main, which outputs one file for each subject with PCI and LZc in /Example/Data/Outputs/Measures/Filtered/.\n",
        "7.   Run the last code chunk to aggregate the outputs across all subjects into one file for analysis in R.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LExb3me7dAtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library installations"
      ],
      "metadata": {
        "id": "KHbF4rBBzQqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo2hRTPe3Hlp"
      },
      "outputs": [],
      "source": [
        "!pip install lempel_ziv_complexity\n",
        "from lempel_ziv_complexity import lempel_ziv_complexity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/renzocom/PCIst.git\n",
        "from PCIst import pci_st"
      ],
      "metadata": {
        "id": "RZE0zhvFu9Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjM41cZKzu-o"
      },
      "source": [
        "# Python imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDZc_spczuPr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import linalg\n",
        "from scipy.signal import hilbert\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g96UBrn2ueox"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxI_PiEQsOmh"
      },
      "source": [
        "# Helper functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5lUOwvDP7_b"
      },
      "source": [
        "## Helper function to compute LZc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyAeUWgOP8Fq"
      },
      "outputs": [],
      "source": [
        "def compute_LZc(np_signal_windowed, baseline_end_absolute, response_start_absolute):\n",
        "  \"\"\"\n",
        "  Computes LZc for a single trial based on a baseline and response window;\n",
        "    returns the result.\n",
        "\n",
        "  Args:\n",
        "    np_signal_windowed (2-d np array of floats): EEG activation values (channels x samples) windowed around a given event.\n",
        "    baseline_end_absolute (int): The absolute (positive) timepoint when baseline periods end, eg 90 for 350ms baselines and a 256 sample rate.\n",
        "    response_start_absolute (int): The absolute (positive) timepoint when response periods start, eg 104 for 350ms baselines (+50ms buffers) and a 256 sample rate.\n",
        "\n",
        "  Returns:\n",
        "    (float): LZc for a single trial.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create empty matrix for binarized data\n",
        "  ncol = len(np_signal_windowed[0, response_start_absolute:])                   # For computing LZc on response data\n",
        "  binarized_matrix = np.empty((0, ncol))\n",
        "  lzc_per_channel = []                                                          # For computing LZc per channel (and taking the average)\n",
        "\n",
        "  # Iterate all channels in EEG data\n",
        "  for channel, signal in enumerate(np_signal_windowed):\n",
        "    # EEG values are binarized using the (mean of the) instantaneous amplitude (absolute value) of the analytical (Hilbert-transformed) signal\n",
        "    # per https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0133532\n",
        "\n",
        "    # Start by hilbert-transforming the EEG data to get the analytical values\n",
        "    hilbert_values_baseline = hilbert(signal[:baseline_end_absolute])           # For binarizing LZc on baseline data\n",
        "    hilbert_values_response = hilbert(signal[response_start_absolute:])         # For computing LZc on response data\n",
        "\n",
        "    # Find the instantaneous amplitude by taking the absolute value of the analytic values\n",
        "    instantaneous_amplitude_values_baseline = np.abs(hilbert_values_baseline)   # For binarizing LZc on baseline data\n",
        "    instantaneous_amplitude_values_response = np.abs(hilbert_values_response)   # For computing LZc on response data\n",
        "\n",
        "    # Take the mean of the instantaneous amplitude\n",
        "    mean_amplitude = np.mean(instantaneous_amplitude_values_baseline)\n",
        "\n",
        "    # Binarize the instantaneous amplitude values based on the mean\n",
        "    binarized_values = np.where(instantaneous_amplitude_values_response > mean_amplitude, 1, 0)\n",
        "\n",
        "    # Add this binarized channel to the final binarized matrix\n",
        "    binarized_matrix = np.vstack([binarized_matrix, binarized_values])\n",
        "\n",
        "    # Compute LZc per channel\n",
        "    flattened_string = \"\".join(binarized_values.astype(str))                    # Equivalent to .join(map(str, arr))\n",
        "    lzc_per_channel.append(lempel_ziv_complexity(flattened_string))\n",
        "\n",
        "  # Flatten the matrix timepoint by timepoint ('F' for column-major), for conversion to string for LZc calculation\n",
        "  flattened_matrix_time = binarized_matrix.flatten('F')\n",
        "\n",
        "  # Flatten the matrix channel by channel, for conversion to string for LZc calculation\n",
        "  flattened_matrix_space = binarized_matrix.flatten()\n",
        "\n",
        "  # Convert the flattened matrix to a binary string for LZc calculation\n",
        "  binary_string_time = \"\".join(flattened_matrix_time.astype(int).flatten().astype(str))   # The order of astype() and flatten() can matter for non-binary values (per ChatGPT)\n",
        "  binary_string_space = \"\".join(flattened_matrix_space.astype(int).flatten().astype(str))   # The order of astype() and flatten() can matter for non-binary values (per ChatGPT)\n",
        "\n",
        "  # Calculate LZc\n",
        "  lzct = lempel_ziv_complexity(binary_string_time)                                     # Alternative LZc implementation, but need to convert to single value: https://rosettacode.org/wiki/LZW_compression#Python\n",
        "  lzcs = lempel_ziv_complexity(binary_string_space)\n",
        "\n",
        "  # Return all 3 LZc values\n",
        "  return (lzct, lzcs, lzc_per_channel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_rjvW00AeRQ"
      },
      "source": [
        "## Helper function to compute PCI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT2Ss-Gd3_jd"
      },
      "outputs": [],
      "source": [
        "def compute_PCI(row, np_signal, baseline_start_factor, baseline_buffer, response_start_factor,\n",
        "                response_end_factor, snr_param, k_param):\n",
        "  \"\"\"\n",
        "  Computes PCI for a single trial based on a baseline and response window;\n",
        "    returns the result.\n",
        "\n",
        "  Args:\n",
        "    row (dictionary): EventList and Timepoint data for a single trial.\n",
        "    np_signal (2-d np array of floats): All EEG activation values (channels x samples) per subject.\n",
        "    baseline_start_factor (int): The event-relative timepoint when baseline periods start, eg -103 for 400ms baselines and a 256 sample rate.\n",
        "    baseline_buffer (int): Number of samples for buffer between baseline_end and response_start, eg 13 for 50ms and a 256 sample rate.\n",
        "    response_start_factor: The event-relative timepoint when response periods start, eg typically 0, unless dividing the response into multiple windows.\n",
        "    response_end_factor (int): The event-relative timepoint when response periods end, eg 103 for 400ms responses and a 256 sample rate.\n",
        "    snr_param (float): Selects principal components with a signal-to-noise ratio (SNR) > min_snr (for PCI).\n",
        "    k_param (float > 1): PCI noise control parameter.\n",
        "\n",
        "  Returns:\n",
        "    (float): PCI for a single trial.\n",
        "  \"\"\"\n",
        "\n",
        "  # Set parameters for PCI calculation\n",
        "  # To offset the response window from the baseline window by 1 timepoint\n",
        "  alignment_buffer = 1\n",
        "\n",
        "  # Compute the absolute (positive) boundaries of the baseline and response windows for the PCI par values\n",
        "  baseline_end_absolute = int((baseline_start_factor * -alignment_buffer) - baseline_buffer)\n",
        "  response_start_absolute = int(baseline_end_absolute + baseline_buffer + alignment_buffer + response_start_factor)\n",
        "  response_end_absolute = int(baseline_end_absolute + baseline_buffer + alignment_buffer + response_end_factor)\n",
        "\n",
        "  par = {'baseline_window':(0, baseline_end_absolute), 'response_window':(response_start_absolute, response_end_absolute),\n",
        "         'k': k_param, 'min_snr': snr_param, 'max_var': 99, 'embed': False, 'n_steps': 100}\n",
        "\n",
        "  # Pull out the actual window start and end timepoints from the eventlist data\n",
        "  baseline_start_timepoint = math.floor(row['baseline_start_timepoint'])\n",
        "  response_end_timepoint = math.floor(row['response_end_timepoint'])\n",
        "\n",
        "  # Create an np array for the activation data from all channels for just the corresponding window\n",
        "  np_signal_windowed = np_signal[:, baseline_start_timepoint:response_end_timepoint]\n",
        "\n",
        "  # Create an np array for just the corresponding timepoints for the PCI calculation\n",
        "  np_timepoints = np.arange(np_signal_windowed.shape[1])\n",
        "\n",
        "  # Calculate PCI\n",
        "  return (pci_st.calc_PCIst(np_signal_windowed, np_timepoints, **par),\n",
        "          np_signal_windowed, par, baseline_end_absolute, baseline_buffer, response_start_absolute,\n",
        "          response_end_absolute)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_r83pQJ56av"
      },
      "source": [
        "## Helper function to compute both measures (PCI and LZc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YMqGW9h56ih"
      },
      "outputs": [],
      "source": [
        "def compute_measures(df_onset_times_binned_timepoints, np_signal, file, baseline_start_factor,\n",
        "                     baseline_buffer, response_start_factor, response_end_factor,\n",
        "                     output_path_measures, input_output_path_measures_filtered, this_subject_string,\n",
        "                     snr_param, k_param, n_channels, dataset, sample_rate, session):\n",
        "  \"\"\"\n",
        "  Computes PCI and LZc for all trials (per subject);\n",
        "    saves the results (CSVs) to Google Drive.\n",
        "\n",
        "  Args:\n",
        "    df_onset_times_binned_timepoints (df): EventList and timepoint data for all trials (per subject).\n",
        "    np_signal (2-d np array of floats): All EEG activation values (channels x samples) per subject (for windowing).\n",
        "    file (string): File name of the current file.\n",
        "    baseline_start_factor (int): The event-relative timepoint when baseline periods start, eg -103 for 400ms baselines and a 256 sample rate.\n",
        "        Passed to computePCI().\n",
        "    baseline_buffer (int): Number of samples for buffer between baseline_end and response_start, eg 13 for 50ms and a 256 sample rate.\n",
        "        Passed to compute_PCI().\n",
        "    response_start_factor: The event-relative timepoint when response periods start, eg typically 0, unless dividing the response into multiple windows.\n",
        "        Passed to compute_PCI().\n",
        "    response_end_factor (int): The event-relative timepoint when response periods end, eg 103 for 400ms responses and a 256 sample rate.\n",
        "        Passed to computePCI().\n",
        "    output_path_measures (string): Google Drive path for outputting PCI/LZc results.\n",
        "    input_output_path_measures_filtered (string): Google Drive path for outputting results filtered to core columns.\n",
        "    this_subject_string (string): ID of current subject.\n",
        "    snr_param (float): Selects principal components with a signal-to-noise ratio (SNR) > min_snr (for PCI).\n",
        "        Passed to computePCI().\n",
        "    k_param (float > 1): PCI noise control parameter.\n",
        "        Passed to computePCI().\n",
        "    n_channels (int): Number of EEG channels specified (for output filename).\n",
        "    dataset: Name of dataset (for output filename).\n",
        "    sample_rate (int): EEG sample rate (for timepoint conversion).\n",
        "    session (string): Session number for filename outputs.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create new df for final PCI/LZc results\n",
        "  df_measures = df_onset_times_binned_timepoints\n",
        "  df_measures[\"lzc_per_channel\"] = \"\"                                           # Initialize column for lzc_per_channel to type string\n",
        "\n",
        "  # Loop through all events in the eventlist data\n",
        "  for event, row in df_onset_times_binned_timepoints.iterrows():\n",
        "\n",
        "    # Compute PCI\n",
        "    (pci, np_signal_windowed, par,\n",
        "     baseline_end_absolute, baseline_buffer,\n",
        "     response_start_absolute, response_end_absolute) = compute_PCI(row, np_signal,\n",
        "                                                                   baseline_start_factor,\n",
        "                                                                   baseline_buffer,\n",
        "                                                                   response_start_factor,\n",
        "                                                                   response_end_factor,\n",
        "                                                                   snr_param, k_param)\n",
        "\n",
        "    # Skip first row if start window goes negative, or last row if end window goes beyond available data\n",
        "    if (np_signal_windowed.size == 0) or (np_signal_windowed[0].size < -baseline_start_factor + response_end_factor):\n",
        "      print(\"WARNING: PCI/LZc WINDOW\", -baseline_start_factor + response_end_factor, \"LARGER THAN SIGNAL\", np_signal_windowed[0].size) # debug\n",
        "      print(\"SETTING PCI AND LZC TO 0 AND CONTINUING TO NEXT ROW/TRIAL/SUBJECT\")\n",
        "      df_measures.loc[event, \"pci\"] = 0\n",
        "      df_measures.loc[event, \"lzct\"] = 0\n",
        "      df_measures.loc[event, \"lzcs\"] = 0\n",
        "      df_measures.loc[event, \"lzc_per_channel\"] = 0\n",
        "\n",
        "    else:\n",
        "\n",
        "      # Add the PCI values to the results df\n",
        "      df_measures.loc[event, \"pci\"] = pci\n",
        "\n",
        "      # Compute LZc\n",
        "      lzct, lzcs, lzc_per_channel = compute_LZc(np_signal_windowed, baseline_end_absolute, response_start_absolute)\n",
        "\n",
        "      # Add the LZc values to the results df\n",
        "      df_measures.loc[event, \"lzct\"] = lzct\n",
        "      df_measures.loc[event, \"lzcs\"] = lzcs\n",
        "      df_measures.loc[event, \"lzc_per_channel\"] = \",\".join(map(str, lzc_per_channel))\n",
        "\n",
        "  # Save new dfs with PCI/LZc results to Google Drive\n",
        "  df_measures.to_csv(output_path_measures + dataset + \"_\" + this_subject_string + session +\n",
        "                     \"_chann\" + str(n_channels) +\n",
        "                     \"_samprate\" + str(sample_rate) +\n",
        "                     \"_base0-\" + str(baseline_end_absolute) +\n",
        "                     \"_resp\" + str(response_start_absolute) + \"-\" + str(response_end_absolute) +\n",
        "                     \"_timepoints_pci_lzc.csv\")\n",
        "\n",
        "  df_measures_filtered = df_measures[[\"ecode\", \"a_flags\", \"bin\", \"pci\", \"lzct\", \"lzcs\", \"lzc_per_channel\"]]\n",
        "  df_measures_filtered.to_csv(input_output_path_measures_filtered + dataset + \"_\" + this_subject_string + session +\n",
        "                              \"_chann\" + str(n_channels) +\n",
        "                              \"_samprate\" + str(sample_rate) +\n",
        "                              \"_base0-\" + str(baseline_end_absolute) +\n",
        "                              \"_resp\" + str(response_start_absolute) + \"-\" + str(response_end_absolute) +\n",
        "                              \"_ecode_aflags_bin_pci_lzc.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QhJ2DdJ2AYF"
      },
      "source": [
        "## Compute timepoints/windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lFtVEog2AkH"
      },
      "outputs": [],
      "source": [
        "def compute_timepoints(df_onset_times_binned, input_output_path_timepoints, sample_rate,\n",
        "                       baseline_start_factor, baseline_buffer, response_start_factor, response_end_factor,\n",
        "                       file, n_channels, this_subject_string, session):\n",
        "  \"\"\"\n",
        "  Converts trial onset times to timepoints (per subject);\n",
        "    computes baseline start and response end timepoints;\n",
        "    saves new dfs to Google Drive.\n",
        "\n",
        "  Args:\n",
        "    df_onset_times_binned (df): EventList data for all trials (per subject).\n",
        "    input_output_path_timepoints (string): Google Drive path for outputting timepoints.\n",
        "    sample_rate (int): EEG sample rate (for timepoint conversion).\n",
        "    baseline_start_factor (int): The event-relative timepoint when baseline periods start, eg -103 for 400ms baselines and a 256 sample rate.\n",
        "    baseline_buffer (int): Number of samples for buffer between baseline_end and response_start, eg 13 for 50ms and a 256 sample rate.\n",
        "    response_end_factor (int): The event-relative timepoint when response periods end, eg 103 for 400ms responses and a 256 sample rate.\n",
        "    response_start_factor: The event-relative timepoint when response periods start, eg typically 0, unless dividing the response into multiple windows.\n",
        "    file (string): File name of the current file.\n",
        "    n_channels (int): Number of EEG channels specified (for output filename).\n",
        "    this_subject_string (string): ID of current subject.\n",
        "    session (string): Session number for filename outputs.\n",
        "\n",
        "  Returns:\n",
        "    df_onset_times_binned_timepoints (df): EventList and timepoint data for all trials (per subject).\n",
        "  \"\"\"\n",
        "\n",
        "  # Add new columns for timepoints and windows\n",
        "  df_onset_times_binned_timepoints = df_onset_times_binned.copy()\n",
        "  df_onset_times_binned_timepoints['timepoint'] = df_onset_times_binned_timepoints['onset'].apply(lambda x: x * sample_rate)\n",
        "  df_onset_times_binned_timepoints['baseline_start_timepoint'] = df_onset_times_binned_timepoints['onset'].apply(lambda x: x * sample_rate + baseline_start_factor)\n",
        "  df_onset_times_binned_timepoints['response_end_timepoint'] = df_onset_times_binned_timepoints['onset'].apply(lambda x: x * sample_rate + response_end_factor)\n",
        "\n",
        "  # Add sanity check that response_end_timepoint ends before next baseline_start_timepoint?\n",
        "\n",
        "  # Save new df with timepoint/window data to Google Drive\n",
        "  df_onset_times_binned_timepoints.to_csv(input_output_path_timepoints + dataset + \"_\" + this_subject_string + session +\n",
        "                                          \"_chann\" + str(n_channels) +\n",
        "                                          \"_base\" + str(baseline_start_factor) + \"_-\" + str(baseline_buffer) +\n",
        "                                          \"_resp\" + str(response_start_factor) + \"-\" + str(response_end_factor) +\n",
        "                                          \"_timepoints.csv\")\n",
        "\n",
        "  return df_onset_times_binned_timepoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNDs0cS32Vjx"
      },
      "source": [
        "## Convert eventlist data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhMR1vEO2VrU"
      },
      "outputs": [],
      "source": [
        "def convert_eventlist_data(dataset, input_path_events, eventlist_files_sorted, this_subject_idx, ignore_rows):\n",
        "  \"\"\"\n",
        "  Converts EventList data from .txt to .csv (including bins).\n",
        "\n",
        "  Args:\n",
        "    dataset: Name of dataset (for output filename).\n",
        "    input_path_events (string): Google Drive path for inputting EventList data (for all subjects).\n",
        "    eventlist_files_sorted (list of strings): List of EventList filenames, sorted increasing to match subject IDs.\n",
        "    this_subject_idx (int): Index of current subject.\n",
        "    ignore_rows (rows): Number of rows to ignore from EventList.txt files.\n",
        "\n",
        "  Returns:\n",
        "    df_onset_times_binned (df): EventList data with bins for all trials (per subject).\n",
        "  \"\"\"\n",
        "\n",
        "  df_onset_times = pd.read_csv(input_path_events + eventlist_files_sorted[this_subject_idx],\n",
        "                               skiprows=ignore_rows, sep=r'\\s+(?![^\\[]*\\])' ,   # split on spaces only when not within brackets, per GPT # sep='\\s+' # delim_whitespace=True deprecated\n",
        "                               header=None,                                     # Ignore header/column names due to formatting inconsistencies\n",
        "                               engine='python')                                 # Specify the python engine because the c engine does not support regex separators\n",
        "\n",
        "  # Remove erroneous column and add correct column names back to the df\n",
        "  df_onset_times.drop(df_onset_times.columns[[9]], axis=1, inplace=True)\n",
        "\n",
        "  df_onset_times.columns =['item', 'bepoch', 'ecode', 'label', 'onset', 'diff', 'dura',\n",
        "                           'b_flags', 'a_flags', 'enable']\n",
        "\n",
        "  \"\"\"\n",
        "  Convert \"enable\" column to bins\n",
        "    Ensure x is a string before using endswith(),\n",
        "    strip the square brackets,\n",
        "    split the resulting string (by spaces),\n",
        "    only if the resulting string isn't empty,\n",
        "    choose the last list item (else set bin to 0)\n",
        "  \"\"\"\n",
        "  df_onset_times_binned = df_onset_times\n",
        "  df_onset_times_binned['bin'] = df_onset_times_binned['enable'].apply(lambda x: 1 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('1')\n",
        "                                                          else (2 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('2')\n",
        "                                                          else (3 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('3')\n",
        "                                                          else (4 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('4')\n",
        "                                                          else (5 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('5')\n",
        "                                                          else (6 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('6')\n",
        "                                                          else (7 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('7')\n",
        "                                                          else (8 if str(x).strip('[]').split() and str(x).strip('[]').split()[-1].endswith('8')\n",
        "                                                          else 0))))))))\n",
        "\n",
        "  return df_onset_times_binned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moP3A9LyrMt9"
      },
      "source": [
        "## Run subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hui-Tro7sOvM"
      },
      "outputs": [],
      "source": [
        "def run_subjects(activation_files_sorted, input_path_activations, dataset, input_path_events,\n",
        "                 eventlist_files_sorted, input_output_path_timepoints, output_path_measures,\n",
        "                 input_output_path_measures_filtered, output_path_general, baseline_start_factor,\n",
        "                 baseline_buffer, response_start_factor, response_end_factor, snr_param,\n",
        "                 k_param, timepoint_files_sorted, measures_filtered_files_sorted, subject_ids,\n",
        "                 sample_rate, n_channels, ignore_rows, session_in_filename):\n",
        "  \"\"\"\n",
        "  Loops through all subjects;\n",
        "    Reads in activation data;\n",
        "    Converts corresponding EventList data;\n",
        "    Computes corresponding timepoints (for PCI/LZc);\n",
        "    Computes PCI/LZC;\n",
        "    Outputs metadata.\n",
        "\n",
        "  Args:\n",
        "    activation_files_sorted (list of strings): List of EEG activation filenames, sorted increasing to match subject IDs.\n",
        "    input_path_activations (string): Google Drive path for inputting Acivations data (for all subjects)\n",
        "    dataset: Name of dataset (for output filename). Passed to convert_eventlist_data().\n",
        "    input_path_events (string): Google Drive path for inputting EventList data (for all subjects).\n",
        "        Passed to convert_eventlist_data().\n",
        "    eventlist_files_sorted (list of strings): List of EventList filenames, sorted increasing to match subject IDs.\n",
        "        Passed to convert_eventlist_data().\n",
        "    input_output_path_timepoints (string): Google Drive path for outputting timepoints.\n",
        "        Passed to compute_timepoints().\n",
        "    output_path_measures (string): Google Drive path for outputting PCI/LZc results.\n",
        "        Passed to compute_measures().\n",
        "    input_output_path_measures_filtered (string): Google Drive path for outputting results filtered to core columns.\n",
        "        Passed to compute_measures().\n",
        "    output_path_general (string): Google Drive path for outputting metadata file.\n",
        "    baseline_start_factor (int): The event-relative timepoint when baseline periods start, eg -103 for 400ms baselines and a 256 sample rate.\n",
        "        Passed to compute_timepoints().\n",
        "    baseline_buffer (int): Number of samples for buffer between baseline_end and response_start, eg 13 for 50ms and a 256 sample rate.\n",
        "        Passed to compute_timepoints().\n",
        "    response_start_factor: The event-relative timepoint when response periods start, eg typically 0, unless dividing the response into multiple windows.\n",
        "        Passed to compute_timepoints().\n",
        "    response_end_factor (int): The event-relative timepoint when response periods end, eg 103 for 400ms responses and a 256 sample rate.\n",
        "        Passed to compute_timepoints().\n",
        "    snr_param (float): Selects principal components with a signal-to-noise ratio (SNR) > min_snr.\n",
        "        Passed to compute_measures().\n",
        "    k_param (float > 1): Noise control parameter. Passed to compute_measures().\n",
        "    timepoint_files_sorted (list of strings): List of timepoint filenames, sorted increasing to match subject IDs.\n",
        "        (If > 0, doesn't re-run convert_eventlist() and compute_timepoints()).\n",
        "    measures_filtered_files_sorted (list of strings):  List of timepoint filenames, sorted increasing to match subject IDs.\n",
        "        (If > 0, doesn't re-run compute_measures().\n",
        "    subject_ids (string): List of subject IDs.\n",
        "    sample_rate (int): EEG sample rate (for timepoint conversion).\n",
        "        Passed to compute_timepoints() and compute_measures().\n",
        "    n_channels (int): Number of EEG channels specified (for output filename).\n",
        "        Passed to compute_timepoints().\n",
        "    session_in_filename: If session number is in filename.\n",
        "\n",
        "  Returns:\n",
        "    (no value): Script completes.\n",
        "  \"\"\"\n",
        "\n",
        "  # For activation-data sanity check and onset-time conversion\n",
        "  seconds_per_minute = 60\n",
        "\n",
        "  # Create lists for metadata\n",
        "  n_subjects_list = []\n",
        "  n_samples = []\n",
        "  n_seconds = []\n",
        "  n_minutes = []\n",
        "\n",
        "  # Loop through all activation/eventlist files (one per subject)\n",
        "  for this_subject_idx, file in enumerate(activation_files_sorted):\n",
        "\n",
        "    print()\n",
        "    print(file)\n",
        "\n",
        "    # Extract subject ID from filename\n",
        "    this_subject_string = file.partition(\"_\")[0]\n",
        "\n",
        "    # Check that this subject is in the list of IDs to process\n",
        "    if this_subject_string not in subject_ids:\n",
        "      print(\"SKIPPING SUBJECT:\", this_subject_string) # debug\n",
        "      continue\n",
        "\n",
        "    # Save subject ID for outputting to filenames\n",
        "    n_subjects_list.append(this_subject_string)\n",
        "\n",
        "    # Read in activation data for one subject\n",
        "    df_activations = pd.read_csv(input_path_activations + file, header=None)\n",
        "\n",
        "    # Convert activation data to numpy array for PCI/LZc calculation\n",
        "    np_signal = df_activations.iloc[:n_channels, :].to_numpy()\n",
        "    print(\"SHAPE (CHANNELS X SAMPLES):\", np_signal.shape) # debug\n",
        "    n_samples.append(np_signal.shape[1])\n",
        "\n",
        "    # Sanity check that there are approximately # minutes worth of data\n",
        "    n_seconds.append(np_signal.shape[1] / sample_rate)\n",
        "    n_minutes.append(np_signal.shape[1] / sample_rate / seconds_per_minute)\n",
        "    print(\"MINUTES OF DATA:\", np_signal.shape[1] / sample_rate / seconds_per_minute) # debug\n",
        "\n",
        "    # Check for session number in filename\n",
        "    if (session_in_filename):\n",
        "      session = \"_\" + file.split(\"_\")[1]\n",
        "    else:\n",
        "      session = \"\"\n",
        "\n",
        "    # Check for existing timepoint data\n",
        "    if(len(timepoint_files_sorted) > 0): # REFACTOR TO CHECK FOR ACTUAL ENTRY\n",
        "      df_onset_times_binned_timepoints = pd.read_csv(input_output_path_timepoints + timepoint_files_sorted[this_subject_idx])\n",
        "\n",
        "    else:\n",
        "      # Convert eventlist data\n",
        "      df_onset_times_binned = convert_eventlist_data(dataset, input_path_events, eventlist_files_sorted, this_subject_idx, ignore_rows)\n",
        "\n",
        "      # Compute timepoints/windows\n",
        "      df_onset_times_binned_timepoints = compute_timepoints(df_onset_times_binned,\n",
        "                                                            input_output_path_timepoints,\n",
        "                                                            sample_rate, baseline_start_factor,\n",
        "                                                            baseline_buffer, response_start_factor,\n",
        "                                                            response_end_factor, file,\n",
        "                                                            n_channels, this_subject_string,\n",
        "                                                            session)\n",
        "\n",
        "    # Add check for existing PCI and LZc data\n",
        "    # Compute PCI and LZc\n",
        "    compute_measures(df_onset_times_binned_timepoints, np_signal, file, baseline_start_factor,\n",
        "                    baseline_buffer, response_start_factor, response_end_factor, output_path_measures,\n",
        "                    input_output_path_measures_filtered, this_subject_string, snr_param, k_param,\n",
        "                    n_channels, dataset, sample_rate, session)\n",
        "\n",
        "  # Create metadata df and output to CSV\n",
        "  df_metadata = pd.DataFrame({'subject_id': n_subjects_list,\n",
        "                              'n_samples': n_samples,\n",
        "                              'n_seconds': n_seconds,\n",
        "                              'n_minutes': n_minutes})\n",
        "  df_metadata.to_csv(output_path_general + dataset + \"_subject_metadata.csv\")\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrLMBDTAsoY"
      },
      "source": [
        "## Load data/files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIFwkRwJZeQ4"
      },
      "outputs": [],
      "source": [
        "def load_data(input_path_activations, input_path_events, input_output_path_timepoints,\n",
        "              input_output_path_measures_filtered):\n",
        "  \"\"\"\n",
        "  Loads EEG activation, Eventlist, and timepoint data from Google Drive.\n",
        "\n",
        "  Args:\n",
        "    input_path_activations (string): Google Drive path for inputting Acivations data (for all subjects)\n",
        "    input_path_events (string): Google Drive path for inputting EventList data (for all subjects).\n",
        "    input_output_path_timepoints (string): Google Drive path for outputting timepoints.\n",
        "    input_output_path_measures_filtered (string): Google Drive path for outputting results filtered to core columns.\n",
        "\n",
        "  Returns:\n",
        "    activation_files_sorted (list of strings): List of EEG activation filenames, sorted increasing to match subject IDs.\n",
        "    eventlist_files_sorted (list of strings): List of EventList filenames, sorted increasing to match subject IDs.\n",
        "    timepoint_files_sorted (list of strings): List of timepoint filenames, sorted increasing to match subject IDs.\n",
        "    measures_filtered_files_sorted (list of strings): List of measures filenames, sorted increasing to match subject IDs.\n",
        "  \"\"\"\n",
        "\n",
        "  # Grab all activation, eventlist, and timepoint files, and sort alphabetically\n",
        "  activation_files = [x for x in os.listdir(input_path_activations)]\n",
        "  eventlist_files = [x for x in os.listdir(input_path_events)]\n",
        "  timepoint_files = [x for x in os.listdir(input_output_path_timepoints)]\n",
        "  measures_filtered_files = [x for x in os.listdir(input_output_path_measures_filtered)]\n",
        "\n",
        "  # If the number of files don't match, stop running\n",
        "  if(len(activation_files) != len(eventlist_files)):\n",
        "    raise ValueError('MISMATCHING FILE NUMBERS!')\n",
        "\n",
        "  # Check filenames for whether to sort by first digit or first+second digits\n",
        "  if activation_files[0].split(\"_\")[1].isdigit():\n",
        "    activation_files_sorted = sorted(activation_files, key=lambda x: (int(x.split(\"_\")[0]), int(x.split(\"_\")[1])))\n",
        "    eventlist_files_sorted = sorted(eventlist_files, key=lambda x: (int(x.split(\"_\")[0]), int(x.split(\"_\")[1])))\n",
        "    timepoint_files_sorted = sorted(timepoint_files, key=lambda x: (int(x.split(\"_\")[1]), int(x.split(\"_\")[2])))\n",
        "    measures_filtered_files_sorted = sorted(measures_filtered_files, key=lambda x: (int(x.split(\"_\")[1]), int(x.split(\"_\")[2])))\n",
        "  else:\n",
        "    activation_files_sorted = sorted(activation_files, key=lambda x: int(x.split(\"_\")[0]))\n",
        "    eventlist_files_sorted = sorted(eventlist_files, key=lambda x: int(x.split(\"_\")[0]))\n",
        "    timepoint_files_sorted = sorted(timepoint_files, key=lambda x: int(x.split(\"_\")[0]))\n",
        "    measures_filtered_files_sorted = sorted(measures_filtered_files, key=lambda x: int(x.split(\"_\")[0]))\n",
        "\n",
        "  return (activation_files_sorted, eventlist_files_sorted, timepoint_files_sorted, measures_filtered_files_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4mBZLCStFCp"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure"
      ],
      "metadata": {
        "id": "5JxRSyO6FfJw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuvt0jtctDle"
      },
      "outputs": [],
      "source": [
        "# Set parameters for dataset\n",
        "file_path_modifier = \"\"                                                         # eg \"Research Projects/\"\n",
        "dataset = \"MMN\"                                                                 # eg N170, P3, MMN; Ekman\n",
        "session_in_filename = False                                                     # If session number is in filename, eg False for N170; True for Ekman\n",
        "subject_ids = list(map(str, range(1, 41)))                                      # eg 1-41 for N170, P3, MMN; 351-389 for Ekman\n",
        "n_channels = 28                                                                 # eg 28 for N170, P3, MMN; 19 for Ekman\n",
        "sample_rate = 256                                                               # eg 256 for N170, P3, MMN; 512 for Ekman\n",
        "\n",
        "# Rows to ignore in Eventlist.txt files\n",
        "ignore_rows = 28                                                                # eg 30 for N170; 28 for P3, MMN; 36 Ekman\n",
        "\n",
        "# Set PCI window parameter values for conversion to timepoints\n",
        "baseline_start_ms = -293                                                        # PCIst default -400; -293 for MMN\n",
        "baseline_buffer_ms = 50                                                         # PCIst default 50\n",
        "response_start_ms = 0                                                           # PCI default 0\n",
        "response_end_ms = 256                                                           # PCIst default 300, + 100 ms for visual sensory processing (eg P100); 256 for MMN\n",
        "print(\"BASELINE AND RESPONSE WINDOWS IN MS:\", baseline_start_ms, 0-baseline_buffer_ms, response_start_ms, response_end_ms)\n",
        "\n",
        "# Subtract samples for the buffer between baseline end and response start\n",
        "baseline_buffer = math.ceil((baseline_buffer_ms / 1000) * sample_rate)\n",
        "\n",
        "# Convert PCI window parameter values to 0-relative timepoint factors\n",
        "baseline_start_factor = math.floor(baseline_start_ms / 1000 * sample_rate)\n",
        "response_start_factor = math.ceil(response_start_ms / 1000 * sample_rate)\n",
        "response_end_factor = math.ceil((response_end_ms) / 1000 * sample_rate)\n",
        "print(\"BASELINE AND RESPONSE WINDOW TIMEPOINT CONVERSION FACTORS:\", baseline_start_factor, 0-baseline_buffer, response_start_factor, response_end_factor)\n",
        "\n",
        "\"\"\"\n",
        "The following parameters may be suboptimal for evoked signals with lower signal-to-noise ratio,\n",
        "  such as those produced by peripheral stimulation, where it may be necessary to increase k\n",
        "  and/or the minimum SNR (1.8 per Alessandra Dallavecchia) to control for stationary baseline-like activations.\n",
        "\"\"\"\n",
        "snr_param = 1.1                                                                 # (float) Selects principal components with a signal-to-noise ratio (SNR) > min_snr (for PCI)\n",
        "k_param = 1.2                                                                   # (float > 1): PCI noise control parameter\n",
        "\n",
        "# Set Google Drive locations for raw data files\n",
        "input_path_activations = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Inputs/Activations/\"\n",
        "input_path_events = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Inputs/Events/\"\n",
        "input_output_path_timepoints = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Outputs/Timepoints/\"\n",
        "output_path_measures = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Outputs/Measures/\"\n",
        "input_output_path_measures_filtered = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Outputs/Measures/Filtered/\"\n",
        "output_path_general = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Outputs/\"\n",
        "\n",
        "# Initialize empty lists for sorted files\n",
        "activation_files_sorted = []\n",
        "eventlist_files_sorted = []\n",
        "timepoint_files_sorted = []\n",
        "\n",
        "# Load data\n",
        "(activation_files_sorted, eventlist_files_sorted, timepoint_files_sorted, measures_filtered_files_sorted) = load_data(input_path_activations,\n",
        "                                                                                                                      input_path_events,\n",
        "                                                                                                                      input_output_path_timepoints,\n",
        "                                                                                                                      input_output_path_measures_filtered)\n",
        "print(activation_files_sorted)\n",
        "print(eventlist_files_sorted)\n",
        "print(timepoint_files_sorted)\n",
        "print(measures_filtered_files_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "71F3XVRWDByo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function call\n",
        "run_subjects(activation_files_sorted, input_path_activations, dataset, input_path_events,\n",
        "             eventlist_files_sorted, input_output_path_timepoints, output_path_measures,\n",
        "             input_output_path_measures_filtered, output_path_general, baseline_start_factor,\n",
        "             baseline_buffer, response_start_factor, response_end_factor, snr_param,\n",
        "             k_param, timepoint_files_sorted, measures_filtered_files_sorted, subject_ids,\n",
        "             sample_rate, n_channels, ignore_rows, session_in_filename)"
      ],
      "metadata": {
        "id": "Keql1IpKMuUt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCHBDzkXtjyx"
      },
      "source": [
        "# Output subject-aggreated df for statistical analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTBOizn9dpWJ"
      },
      "outputs": [],
      "source": [
        "# Read back in trial-by-trial PCI results and sort by subject id\n",
        "input_output_measures_filtered = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Outputs/Measures/Filtered/\"\n",
        "output_path_general = \"/content/drive/MyDrive/\" + file_path_modifier + dataset + \"/Data/Outputs/\"\n",
        "results_files = [x for x in os.listdir(input_output_measures_filtered)]\n",
        "results_files_sorted = sorted(results_files, key=lambda x: int(x.split(\"_\")[1]))\n",
        "\n",
        "# Initialize new df for aggregate of all subjects\n",
        "aggregated_df = pd.DataFrame()\n",
        "\n",
        "# Loop through all subjects\n",
        "for subject, file in enumerate(results_files_sorted):\n",
        "\n",
        "  # Read in results, rename trial column, and add subject_id and session columns\n",
        "  df_pci_filtered = pd.read_csv(input_output_measures_filtered + file)\n",
        "  df_pci_filtered.rename(columns = {'Unnamed: 0':'trial'}, inplace=True)\n",
        "  df_pci_filtered.rename(columns = {'bin':'condition'}, inplace=True)\n",
        "  df_pci_filtered['subject_id'] = file.split(\"_\")[1]\n",
        "  if session_in_filename:\n",
        "    df_pci_filtered['session'] = file.split(\"_\")[2]\n",
        "  aggregated_df = pd.concat([aggregated_df, df_pci_filtered])\n",
        "\n",
        "# Reset a new sequential index\n",
        "aggregated_df = aggregated_df.reset_index(drop=True)\n",
        "\n",
        "# Save final aggregated csv for statistical analysis\n",
        "aggregated_df.to_csv(output_path_general + dataset + \"_aggregated\" +\n",
        "                     \"_chann\" + str(n_channels) +\n",
        "                     \"_samprate\" + str(sample_rate) +\n",
        "                     \"_base\" + str(baseline_start_factor) + \"_-\" + str(baseline_buffer) +\n",
        "                     \"_resp\" + str(response_start_factor) + \"-\" + str(response_end_factor) +\n",
        "                     \"_trial_ecode_aflags_condition_pci_lzc_subjectID.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KHbF4rBBzQqp",
        "tjM41cZKzu-o",
        "M5lUOwvDP7_b",
        "c_rjvW00AeRQ",
        "l_r83pQJ56av",
        "_QhJ2DdJ2AYF",
        "mNDs0cS32Vjx",
        "moP3A9LyrMt9",
        "NWrLMBDTAsoY",
        "5JxRSyO6FfJw"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNZNXU7PczbZf6G/XyRS4H7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}