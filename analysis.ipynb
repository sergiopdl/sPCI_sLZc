{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiopdl/sPCI_sLZc/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#READ ME\n",
        "\n",
        "1.   Create the following directories in the root of your Google Drive (replacing \"Example\" with any desired name of your project):\n",
        "*  /Example/Data/Inputs/Activations/\n",
        "*  /Example/Data/Inputs/Events/\n",
        "*  /Example/Data/Outputs/Timepoints/\n",
        "*  /Example/Data/Outputs/Measures/\n",
        "*  /Example/Data/Outputs/Measures/Filtered/\n",
        "\n",
        "2.   Put your 2x2 activation matrix .CSVs (channels x timepoints, 1 file per subject) in /Example/Data/Inputs/Activations/.\n",
        "3.   Put your corresponding EventList .TXTs (1 per subject) in /Example/Data/Inputs/Events/.\n",
        "4.   Run each code chunk from top to bottom (PCI code, Python imports, and helper functions) until you get to Main.\n",
        "5.  In the first code chunk in Main, configure the parameters accordingly.\n",
        "6.   Run the second code in chunk Main, which outputs one file for each subject with PCI and LZc in /Example/Data/Outputs/Measures/Filtered/.\n",
        "7.   Run the last code chunk to aggregate the outputs across all subjects into one file for analysis in R.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LExb3me7dAtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library installations"
      ],
      "metadata": {
        "id": "KHbF4rBBzQqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo2hRTPe3Hlp"
      },
      "outputs": [],
      "source": [
        "!pip install lempel_ziv_complexity\n",
        "from lempel_ziv_complexity import lempel_ziv_complexity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/renzocom/PCIst.git\n",
        "from PCIst import pci_st"
      ],
      "metadata": {
        "id": "RZE0zhvFu9Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjM41cZKzu-o"
      },
      "source": [
        "# Python imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "bDZc_spczuPr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import linalg\n",
        "from scipy.signal import hilbert\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g96UBrn2ueox"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxI_PiEQsOmh"
      },
      "source": [
        "# Helper functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5lUOwvDP7_b"
      },
      "source": [
        "## Helper function to compute LZc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "CyAeUWgOP8Fq"
      },
      "outputs": [],
      "source": [
        "def compute_LZc(np_signal_windowed, response_start_absolute, baseline_end_absolute):\n",
        "  \"\"\"\n",
        "  Computes LZc for a single trial based on a baseline and response window;\n",
        "    returns the result.\n",
        "\n",
        "  Args:\n",
        "    np_signal_windowed (2-d np array of floats): EEG activation values (channels x samples) windowed around a given event.\n",
        "    response_start_absolute (int): The absolute timepoint when response periods start, eg 104 for 400ms baselines.\n",
        "    baseline_end_absolute (int): The absolute timepoint when baseline periods end, eg 90 for 400ms baselines.\n",
        "\n",
        "  Returns:\n",
        "    (float): LZc for a single trial.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create empty matrix for binarized data\n",
        "  ncol = len(np_signal_windowed[0, response_start_absolute:])                   # For computing LZc on response data\n",
        "  binarized_matrix = np.empty((0, ncol))\n",
        "  lzc_per_channel = []                                                          # For computing LZc per channel (and taking the average)\n",
        "\n",
        "  # Iterate all channels in EEG data\n",
        "  for channel, signal in enumerate(np_signal_windowed):\n",
        "    # EEG values are binarized using the (mean of the) instantaneous amplitude (absolute value) of the analytical (Hilbert-transformed) signal\n",
        "    # per https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0133532\n",
        "\n",
        "    # Start by hilbert-transforming the EEG data to get the analytical values\n",
        "    hilbert_values_baseline = hilbert(signal[:baseline_end_absolute])           # For binarizing LZc on baseline data\n",
        "    hilbert_values_response = hilbert(signal[response_start_absolute:])         # For computing LZc on response data\n",
        "\n",
        "    # Find the \"instantaneous amplitude\" by taking the absolute value of the analytic values\n",
        "    instantaneous_amplitude_values_baseline = np.abs(hilbert_values_baseline)   # For binarizing LZc on baseline data\n",
        "    instantaneous_amplitude_values_response = np.abs(hilbert_values_response)   # For computing LZc on response data\n",
        "\n",
        "    # Take the mean of the instantaneous amplitude\n",
        "    mean_amplitude = np.mean(instantaneous_amplitude_values_baseline)\n",
        "\n",
        "    # Binarize the instantaneous amplitude values based on the mean\n",
        "    binarized_values = np.where(instantaneous_amplitude_values_response > mean_amplitude, 1, 0)\n",
        "\n",
        "    # Add this binarized channel to the final binarized matrix\n",
        "    binarized_matrix = np.vstack([binarized_matrix, binarized_values])\n",
        "\n",
        "    # Compute LZc per channel (and take the average)\n",
        "    # flattened_string = \"\".join(binarized_values.astype(str))                  # Equivalent to .join(map(str, arr))\n",
        "    # lzc_per_channel.append(lempel_ziv_complexity(flattened_string))\n",
        "\n",
        "  # Flatten the matrix (timepoint by timepoint, 'F' for column-major) for conversion to string for LZc calculation\n",
        "  flattened_matrix = binarized_matrix.flatten('F')\n",
        "\n",
        "  # Convert the flattened matrix to a binary string for LZc calculation\n",
        "  binary_string = \"\".join(flattened_matrix.astype(int).flatten().astype(str))   # The order of astype() and flatten() can matter for non-binary values (per ChatGPT)\n",
        "\n",
        "  # Calculate LZc\n",
        "  return lempel_ziv_complexity(binary_string)                                   # Alternative LZc implementation, but need to convert to single value: https://rosettacode.org/wiki/LZW_compression#Python\n",
        "\n",
        "  # Calculate average LZc (per channel)\n",
        "  # return statistics.mean(lempel_values_per_channel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_rjvW00AeRQ"
      },
      "source": [
        "## Helper function to compute PCI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "JT2Ss-Gd3_jd"
      },
      "outputs": [],
      "source": [
        "def compute_PCI(row, np_signal, baseline_start_factor, response_end_factor, snr_param, k_param, baseline_buffer):\n",
        "  \"\"\"\n",
        "  Computes PCI for a single trial based on a baseline and response window;\n",
        "    returns the result.\n",
        "\n",
        "  Args:\n",
        "    row (dictionary): EventList and Timepoint data for a single trial.\n",
        "    np_signal (2-d np array of floats): All EEG activation values (channels x samples) per subject.\n",
        "    baseline_start_factor (int): The relative timepoint when baseline periods start, eg -103 for 400ms baselines.\n",
        "    response_end_factor (int): The relative timepoint when response periods end, eg 90 for 400ms responses.\n",
        "    snr_param (float): Selects principal components with a signal-to-noise ratio (SNR) > min_snr.\n",
        "    k_param (float > 1): Noise control parameter.\n",
        "    baseline_buffer: Number of samples for a 50ms buffer between baseline_end and response_start\n",
        "\n",
        "  Returns:\n",
        "    (float): PCI for a single trial.\n",
        "  \"\"\"\n",
        "\n",
        "  # Set parameters for PCI calculation\n",
        "  # To offset the response window from the baseline window by 1 timepoint\n",
        "  alignment_buffer = 1\n",
        "\n",
        "  # Compute \"absolute\" edges of the baseline and response windows for the PCI par values\n",
        "  baseline_end_absolute = int((baseline_start_factor * -alignment_buffer) - baseline_buffer)\n",
        "  response_start_absolute = int(baseline_end_absolute + alignment_buffer + baseline_buffer)\n",
        "  response_end_absolute = int(response_start_absolute + response_end_factor)\n",
        "  par = {'baseline_window':(0, baseline_end_absolute), 'response_window':(response_start_absolute, response_end_absolute),\n",
        "         'k': k_param, 'min_snr': snr_param, 'max_var': 99, 'embed': False, 'n_steps': 100}\n",
        "\n",
        "  # Pull out the actual window start and end timepoints from the eventlist data\n",
        "  baseline_start_timepoint = math.floor(row['baseline_start_timepoint'])\n",
        "  response_end_timepoint = math.floor(row['response_end_timepoint'])\n",
        "\n",
        "  # Create an np array for the activation data from all channels for just the corresponding window\n",
        "  np_signal_windowed = np_signal[:, baseline_start_timepoint:response_end_timepoint]\n",
        "\n",
        "  # Create an np array for just the corresponding timepoints for the PCI calculation\n",
        "  np_timepoints = np.arange(np_signal_windowed.shape[1])\n",
        "\n",
        "  # Calculate PCI\n",
        "  return (pci_st.calc_PCIst(np_signal_windowed, np_timepoints, **par),\n",
        "          np_signal_windowed, par, response_start_absolute, baseline_end_absolute, baseline_buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_r83pQJ56av"
      },
      "source": [
        "## Helper function to compute both measures (PCI and LZc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "5YMqGW9h56ih"
      },
      "outputs": [],
      "source": [
        "def compute_measures(df_onset_times_binned_timepoints, np_signal, file, baseline_start_factor,\n",
        "                     response_end_factor, output_measures, input_output_measures_filtered,\n",
        "                     subject_string, snr_param, k_param, n_channels, dataset, baseline_buffer):\n",
        "  \"\"\"\n",
        "  Computes PCI and LZc for all trials (per subject);\n",
        "    saves the results (CSVs) to Google Drive.\n",
        "\n",
        "  Args:\n",
        "    df_onset_times_binned_timepoints (df): EventList and timepoint data for all trials (per subject).\n",
        "    np_signal (2-d np array of floats): All EEG activation values (channels x samples) per subject (for windowing).\n",
        "    file (string): File name of the current file.\n",
        "    baseline_start_factor (int): The relative timepoint when baseline periods start, eg -103 for 400ms baselines.\n",
        "    response_end_factor (int): The relative timepoint when response periods end, eg 90 for 400ms responses.\n",
        "    output_measures (string): Google Drive path for outputting PCI/LZc results.\n",
        "    input_output_measures_filtered (string): Google Drive path for outputting results filtered to core columns.\n",
        "    subject_string (int): ID of current subject, incremented by 1 for filename string outputs.\n",
        "    snr_param (float): Selects principal components with a signal-to-noise ratio (SNR) > min_snr.\n",
        "        Passed to computePCI().\n",
        "    k_param (float > 1): Noise control parameter.\n",
        "        Passed to computePCI().\n",
        "    n_channels (int): Number of EEG channels specified (for output filename).\n",
        "    dataset: Name of dataset (for output filename).\n",
        "    baseline_buffer: Number of samples for a 50ms buffer between baseline_end and response_start\n",
        "        Passed to compute_PCI().\n",
        "  \"\"\"\n",
        "\n",
        "  # Create new df for final PCI/LZc results\n",
        "  df_measures = df_onset_times_binned_timepoints\n",
        "\n",
        "  # Loop through all events in the eventlist data\n",
        "  for event, row in df_onset_times_binned_timepoints.iterrows():\n",
        "\n",
        "    # Compute PCI\n",
        "    pci, np_signal_windowed, par, response_start_absolute, baseline_end_absolute, baseline_buffer = compute_PCI(row,\n",
        "                                                                                                 np_signal,\n",
        "                                                                                                 baseline_start_factor,\n",
        "                                                                                                 response_end_factor,\n",
        "                                                                                                 snr_param,\n",
        "                                                                                                 k_param,\n",
        "                                                                                                 baseline_buffer)\n",
        "\n",
        "    # Skip first row if start window goes negative, or last row if end window goes beyond available data\n",
        "    if (np_signal_windowed.size == 0) or (np_signal_windowed[0].size < -baseline_start_factor + response_end_factor):\n",
        "      print(\"ERROR: PCI/LZc WINDOW LARGER THAN SIGNAL\", -baseline_start_factor + response_end_factor, np_signal_windowed[0].size)\n",
        "      print(\"SETTING PCI AND LZC TO 0 AND CONTINUING TO NEXT ROW/TRIAL/SUBJECT\")\n",
        "      df_measures.loc[event, \"pci\"] = 0\n",
        "      df_measures.loc[event, \"lzc\"] = 0\n",
        "\n",
        "    else:\n",
        "\n",
        "      # Add the PCI values to the results df\n",
        "      df_measures.loc[event, \"pci\"] = pci\n",
        "\n",
        "      # Compute LZc\n",
        "      lzc = compute_LZc(np_signal_windowed, response_start_absolute, baseline_end_absolute)\n",
        "\n",
        "      # Add the LZc values to the results df\n",
        "      df_measures.loc[event, \"lzc\"] = lzc\n",
        "\n",
        "  # Save new dfs with PCI results data to Google Drive\n",
        "  df_measures.to_csv(output_measures + file[0:-4] +\n",
        "                     \"_chann\" + str(n_channels) +\n",
        "                     \"_base\" + str(baseline_start_factor) + \"_-\" + str(baseline_buffer) +\n",
        "                     \"_resp\" + str(response_end_factor) +\n",
        "                     \"_snr\" + str(snr_param) +\n",
        "                     \"_k\" + str(k_param) +\n",
        "                     \"_timepoints_pci_lzc.csv\")\n",
        "\n",
        "  df_measures_filtered = df_measures[[\"ecode\", \"a_flags\", \"bin\", \"pci\", \"lzc\"]]\n",
        "  df_measures_filtered.to_csv(input_output_measures_filtered + subject_string + \"_\" + dataset +\n",
        "                              \"_chann\" + str(n_channels) +\n",
        "                              \"_base\" + str(baseline_start_factor) + \"_-\" + str(baseline_buffer) +\n",
        "                              \"_resp\" + str(response_end_factor) +\n",
        "                              \"_snr\" + str(snr_param) +\n",
        "                              \"_k\" + str(k_param) +\n",
        "                              \"_ecode_aflags_bin_pci_lzc.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QhJ2DdJ2AYF"
      },
      "source": [
        "## Compute timepoints/windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "2lFtVEog2AkH"
      },
      "outputs": [],
      "source": [
        "def compute_timepoints(df_onset_times_binned, input_output_timepoints, sample_rate,\n",
        "                       baseline_start_factor, response_end_factor, file, n_channels, baseline_buffer):\n",
        "  \"\"\"\n",
        "  Converts trial onset times to timepoints (per subject);\n",
        "    computes baseline start and response end timepoints;\n",
        "    saves new dfs to Google Drive.\n",
        "\n",
        "  Args:\n",
        "    df_onset_times_binned (df): EventList data for all trials (per subject).\n",
        "    input_output_timepoints (string): Google Drive path for outputting timepoints.\n",
        "    sample_rate (int): EEG sample rate (for timepoint conversion).\n",
        "    baseline_start_factor (int): The relative timepoint when baseline periods start, eg -103 for 400ms baselines.\n",
        "    response_end_factor (int): The relative timepoint when response periods end, eg 90 for 300ms responses.\n",
        "    file (string): File name of the current file.\n",
        "    n_channels (int): Number of EEG channels specified (for output filename).\n",
        "    baseline_buffer: Number of samples for a 50ms buffer between baseline_end and response_start\n",
        "\n",
        "  Returns:\n",
        "    df_onset_times_binned_timepoints (df): EventList and timepoint data for all trials (per subject).\n",
        "  \"\"\"\n",
        "\n",
        "  # Add new columns for timepoints and windows\n",
        "  df_onset_times_binned_timepoints = df_onset_times_binned.copy()\n",
        "  df_onset_times_binned_timepoints['timepoint'] = df_onset_times_binned_timepoints['onset'].apply(lambda x: x * sample_rate)\n",
        "  df_onset_times_binned_timepoints['baseline_start_timepoint'] = df_onset_times_binned_timepoints['onset'].apply(lambda x: x * sample_rate + baseline_start_factor)\n",
        "  df_onset_times_binned_timepoints['response_end_timepoint'] = df_onset_times_binned_timepoints['onset'].apply(lambda x: x * sample_rate + response_end_factor)\n",
        "\n",
        "  # Add sanity check that response_end_timepoint ends before next baseline_start_timepoint?\n",
        "\n",
        "  # Save new df with timepoint/window data to Google Drive\n",
        "  df_onset_times_binned_timepoints.to_csv(input_output_timepoints + file[0:-4] +\n",
        "                                          \"_chann\" + str(n_channels) +\n",
        "                                          \"_base\" + str(baseline_start_factor) + \"_-\" + str(baseline_buffer) +\n",
        "                                          \"_resp\" + str(response_end_factor) +\n",
        "                                          \"_timepoints.csv\")\n",
        "\n",
        "  return df_onset_times_binned_timepoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNDs0cS32Vjx"
      },
      "source": [
        "## Convert eventlist data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "mhMR1vEO2VrU"
      },
      "outputs": [],
      "source": [
        "def convert_eventlist_data(dataset, input_events, eventlist_files_sorted, subject_int, ignore_rows):\n",
        "  \"\"\"\n",
        "  Converts EventList data from .txt to .csv (including bins).\n",
        "\n",
        "  Args:\n",
        "    dataset: Name of dataset (for output filename).\n",
        "    input_events (string): Google Drive path for inputting EventList data (for all subjects).\n",
        "    eventlist_files_sorted (list of strings): List of EventList filenames, sorted increasing to match subject IDs.\n",
        "    subject_int (int): ID of current subject.\n",
        "\n",
        "  Returns:\n",
        "    df_onset_times_binned (df): EventList data with bins for all trials (per subject).\n",
        "  \"\"\"\n",
        "\n",
        "  df_onset_times = pd.read_csv(input_events + eventlist_files_sorted[subject_int],\n",
        "                               skiprows=ignore_rows, delim_whitespace=True,\n",
        "                               header=None)                                     # Ignore header/column names due to formatting inconsistencies\n",
        "\n",
        "  # Remove erroneous column and add correct column names back to the df\n",
        "  df_onset_times.drop(df_onset_times.columns[[9,10]], axis=1, inplace=True)\n",
        "  df_onset_times.columns =['item', 'bepoch', 'ecode', 'label', 'onset', 'diff', 'dura',\n",
        "                           'b_flags', 'a_flags', 'enable']\n",
        "\n",
        "  # Convert \"enable\" column to bins\n",
        "  df_onset_times_binned = df_onset_times\n",
        "  df_onset_times_binned['bin'] = df_onset_times_binned['enable'].apply(lambda x: 1 if x.startswith('1')\n",
        "                                                          else (2 if x.startswith('2')\n",
        "                                                          else (3 if x.startswith('3')\n",
        "                                                          else (4 if x.startswith('4')\n",
        "                                                          else 0))))\n",
        "\n",
        "  return df_onset_times_binned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moP3A9LyrMt9"
      },
      "source": [
        "## Run subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Hui-Tro7sOvM"
      },
      "outputs": [],
      "source": [
        "def run_subjects(activation_files_sorted, input_activations, dataset, input_events,\n",
        "                 eventlist_files_sorted, input_output_timepoints, output_measures,\n",
        "                 input_output_measures_filtered, baseline_start_factor, response_end_factor,\n",
        "                 snr_param, k_param, timepoint_files_sorted, n_subjects, sample_rate,\n",
        "                 n_channels, ignore_rows, baseline_buffer):\n",
        "  \"\"\"\n",
        "  Loops through all subjects;\n",
        "    Reads in activation data;\n",
        "    Converts corresponding EventList data;\n",
        "    Computes corresponding timepoints (for PCI/LZc);\n",
        "    Computes PCI/LZC;\n",
        "    Outputs metadata.\n",
        "\n",
        "  Args:\n",
        "    activation_files_sorted (list of strings): List of EEG activation filenames, sorted increasing to match subject IDs.\n",
        "    input_activations (string): Google Drive path for inputting Acivations data (for all subjects)\n",
        "    dataset: Name of dataset (for output filename). Passed to convert_eventlist_data().\n",
        "    input_events (string): Google Drive path for inputting EventList data (for all subjects).\n",
        "        Passed to convert_eventlist_data().\n",
        "    eventlist_files_sorted (list of strings): List of EventList filenames, sorted increasing to match subject IDs.\n",
        "        Passed to convert_eventlist_data().\n",
        "    input_output_timepoints (string): Google Drive path for outputting timepoints.\n",
        "        Passed to compute_timepoints().\n",
        "    output_measures (string): Google Drive path for outputting PCI/LZc results.\n",
        "        Passed to compute_measures().\n",
        "    input_output_measures_filtered (string): Google Drive path for outputting results filtered to core columns.\n",
        "        Passed to compute_measures().\n",
        "    baseline_start_factor (int): The relative timepoint when baseline periods start, eg -103 for 400ms baselines.\n",
        "        Passed to compute_timepoints().\n",
        "    response_end_factor (int): The relative timepoint when response periods end, eg 90 for 400ms responses.\n",
        "        Passed to compute_timepoints().\n",
        "    snr_param (float): Selects principal components with a signal-to-noise ratio (SNR) > min_snr.\n",
        "        Passed to compute_measures().\n",
        "    k_param (float > 1): Noise control parameter. Passed to compute_measures().\n",
        "    timepoint_files_sorted (list of strings): List of timepoint filenames, sorted increasing to match subject IDs.\n",
        "        (If > 0, doesn't re-run convert_eventlist() and compute_timepoints()).\n",
        "    n_subjects (int): Number of subjects specified.\n",
        "    sample_rate (int): EEG sample rate (for timepoint conversion).\n",
        "        Passed to compute_timepoints().\n",
        "    n_channels (int): Number of EEG channels specified (for output filename).\n",
        "        Passed to compute_timepoints().\n",
        "    baseline_buffer: Number of samples for a 50ms buffer between baseline_end and response_start\n",
        "        Passed to compute_measures().\n",
        "\n",
        "  Returns:\n",
        "    (no value): Script compltes.\n",
        "  \"\"\"\n",
        "\n",
        "  # For activation-data sanity check and onset-time conversion\n",
        "  seconds_per_minute = 60\n",
        "\n",
        "  # Create lists for metadata\n",
        "  n_subjects_list = []\n",
        "  n_samples = []\n",
        "  n_seconds = []\n",
        "  n_minutes = []\n",
        "\n",
        "  # Loop through all activation/eventlist files (one per subject)\n",
        "  for subject_int, file in enumerate(activation_files_sorted):\n",
        "\n",
        "    print()\n",
        "    print(file)\n",
        "    # Break after n_subjects for debugging\n",
        "    if (subject_int + 1) == n_subjects:\n",
        "      break\n",
        "\n",
        "    # Save subject ID for outputting to filenames\n",
        "    subject_string = file.partition(\"_\")[0]\n",
        "    n_subjects_list.append(subject_string)\n",
        "\n",
        "    # Read in activation data for one subject\n",
        "    df_activations = pd.read_csv(input_activations + file, header=None)\n",
        "\n",
        "    # Convert activation data to numpy array for PCI/LZc calculation\n",
        "    np_signal = df_activations.iloc[:n_channels, :].to_numpy()\n",
        "    print(\"Shape (channels x samples):\", np_signal.shape) # debug\n",
        "    n_samples.append(np_signal.shape[1])\n",
        "\n",
        "    # Sanity check that there are approximately 10-12 minutes worth of data\n",
        "    #   per: https://www.sciencedirect.com/science/article/pii/S1053811920309502?via%3Dihub\n",
        "    n_seconds.append(np_signal.shape[1] / sample_rate)\n",
        "    n_minutes.append(np_signal.shape[1] / sample_rate / seconds_per_minute)\n",
        "    # print(\"Minutes of data sanity check:\", np_signal.shape[1] / sample_rate / seconds_per_minute) # debug\n",
        "\n",
        "    # Check for existing timepoint data\n",
        "    if(len(timepoint_files_sorted) > 0): # REFACTOR TO CHECK FOR ACTUAL ENTRY\n",
        "      df_onset_times_binned_timepoints = pd.read_csv(input_output_timepoints + timepoint_files_sorted[subject_int])\n",
        "\n",
        "    else:\n",
        "      # Convert eventlist data\n",
        "      df_onset_times_binned = convert_eventlist_data(dataset, input_events, eventlist_files_sorted, subject_int, ignore_rows)\n",
        "\n",
        "      # Compute timepoints/windows\n",
        "      df_onset_times_binned_timepoints = compute_timepoints(df_onset_times_binned,\n",
        "                                                          input_output_timepoints,\n",
        "                                                          sample_rate, baseline_start_factor,\n",
        "                                                          response_end_factor, file, n_channels, baseline_buffer)\n",
        "\n",
        "    # Compute PCI and LZc\n",
        "    compute_measures(df_onset_times_binned_timepoints, np_signal, file, baseline_start_factor,\n",
        "                     response_end_factor, output_measures, input_output_measures_filtered,\n",
        "                     subject_string, snr_param, k_param, n_channels, dataset, baseline_buffer)\n",
        "\n",
        "  # Create metadata df and output to CSV\n",
        "  df_metadata = pd.DataFrame({'n_subjects': len(n_subjects),\n",
        "                              'n_samples': n_samples,\n",
        "                              'n_seconds': n_seconds,\n",
        "                              'n_minutes': n_minutes})\n",
        "  df_metadata.to_csv(output_general + dataset + \"_subject_metadata.csv\")\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrLMBDTAsoY"
      },
      "source": [
        "## Load data/files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "OIFwkRwJZeQ4"
      },
      "outputs": [],
      "source": [
        "def load_data(input_activations, input_events, input_output_timepoints):\n",
        "  \"\"\"\n",
        "  Loads EEG activation, Eventlist, and timepoint data from Google Drive.\n",
        "\n",
        "  Args:\n",
        "    input_activations (string): Google Drive path for inputting Acivations data (for all subjects)\n",
        "    input_events (string): Google Drive path for inputting EventList data (for all subjects).\n",
        "    input_output_timepoints (string): Google Drive path for outputting timepoints.\n",
        "\n",
        "  Returns:\n",
        "    activation_files_sorted (list of strings): List of EEG activation filenames, sorted increasing to match subject IDs.\n",
        "    eventlist_files_sorted (list of strings): List of EventList filenames, sorted increasing to match subject IDs.\n",
        "    timepoint_files_sorted (list of strings): List of timepoint filenames, sorted increasing to match subject IDs.\n",
        "  \"\"\"\n",
        "\n",
        "  # Grab all activation files and sort alphabetically\n",
        "  activation_files = [x for x in os.listdir(input_activations)]\n",
        "  activation_files_sorted = sorted(activation_files, key=lambda x: int(x.partition(\"_\")[0]))\n",
        "\n",
        "  # Grab all eventlist files and sort alphabetically to match order of activation files\n",
        "  eventlist_files = [x for x in os.listdir(input_events)]\n",
        "  eventlist_files_sorted = sorted(eventlist_files, key=lambda x: int(x.partition(\"_\")[0]))\n",
        "\n",
        "  # Grab all timepoint files and sort alphabetically to match order of activation files\n",
        "  timepoint_files = [x for x in os.listdir(input_output_timepoints)]\n",
        "  timepoint_files_sorted = sorted(timepoint_files, key=lambda x: int(x.partition(\"_\")[0]))\n",
        "\n",
        "  # If the number of files don't match, stop running\n",
        "  if(len(activation_files) != len(eventlist_files)):\n",
        "    raise ValueError('Mismatching file numbers!')\n",
        "\n",
        "  return (activation_files_sorted, eventlist_files_sorted, timepoint_files_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4mBZLCStFCp"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure"
      ],
      "metadata": {
        "id": "5JxRSyO6FfJw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuvt0jtctDle"
      },
      "outputs": [],
      "source": [
        "# Set parameters for dataset\n",
        "dataset = \"Example\"\n",
        "n_subjects = 1\n",
        "n_channels = 19\n",
        "sample_rate = 512\n",
        "\n",
        "# Rows to ignore in Eventlist.txt files (30 for N170 ; 28 for P3 and MMN)\n",
        "ignore_rows = 30\n",
        "\n",
        "# Set PCI window parameter values for conversion to timepoints\n",
        "baseline_start_ms = -400 # PCIst default\n",
        "#baseline_end_ms = -50\n",
        "#repsonse_start_ms = 0\n",
        "response_end_ms = 300 + 100 # PCIst default + 100 ms for visual sensory processing (eg P100)\n",
        "\n",
        "# Subtract samples for a 50ms buffer between baseline end and response start (per default PCI par values)\n",
        "baseline_buffer = math.ceil((50 / 1000) * sample_rate)\n",
        "\n",
        "# Convert PCI window parameter values to 0-relative timepoint factors\n",
        "baseline_start_factor = math.floor(baseline_start_ms / 1000 * sample_rate)\n",
        "response_end_factor = math.ceil(response_end_ms / 1000 * sample_rate)\n",
        "\n",
        "\"\"\"\n",
        "The following parameters may be suboptimal for evoked signals with lower signal-to-noise ratio,\n",
        "  such as those produced by peripheral stimulation, where it may be necessary to increase k\n",
        "  and/or the minimum SNR (1.8 per Alessandra Dallavecchia) to control for stationary baseline-like activations.\n",
        "\"\"\"\n",
        "snr_param = 1.1\n",
        "k_param = 1.2\n",
        "\n",
        "# Set Google Drive locations for raw data files\n",
        "input_activations = \"/content/drive/MyDrive/\" + dataset + \"/Data/Inputs/Activations/\"\n",
        "input_events = \"/content/drive/MyDrive/\" + dataset + \"/Data/Inputs/Events/\"\n",
        "input_output_timepoints = \"/content/drive/MyDrive/\" + dataset + \"/Data/Outputs/Timepoints/\"\n",
        "output_measures = \"/content/drive/MyDrive/\" + dataset + \"/Data/Outputs/Measures/\"\n",
        "input_output_measures_filtered = \"/content/drive/MyDrive/\" + dataset + \"/Data/Outputs/Measures/Filtered/\"\n",
        "output_general = \"/content/drive/MyDrive/\" + dataset + \"/Data/\"\n",
        "\n",
        "# Initialize empty lists for sorted files\n",
        "activation_files_sorted = []\n",
        "eventlist_files_sorted = []\n",
        "timepoint_files_sorted = []\n",
        "\n",
        "# Load data\n",
        "(activation_files_sorted, eventlist_files_sorted, timepoint_files_sorted) = load_data(input_activations,\n",
        "                                                                                      input_events,\n",
        "                                                                                      input_output_timepoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "71F3XVRWDByo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function call\n",
        "run_subjects(activation_files_sorted, input_activations, dataset, input_events,\n",
        "             eventlist_files_sorted, input_output_timepoints, output_measures,\n",
        "             input_output_measures_filtered, baseline_start_factor, response_end_factor,\n",
        "             snr_param, k_param, timepoint_files_sorted, n_subjects, sample_rate,\n",
        "             n_channels, ignore_rows, baseline_buffer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Keql1IpKMuUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCHBDzkXtjyx"
      },
      "source": [
        "# Output subject-aggreated df for analysis in R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTBOizn9dpWJ"
      },
      "outputs": [],
      "source": [
        "# Read back in trial-by-trial PCI results and sort by subject id\n",
        "input_output_measures_filtered = \"/content/drive/MyDrive/\" + dataset + \"/Data/Outputs/Measures/Filtered/\"\n",
        "results_files = [x for x in os.listdir(input_output_measures_filtered)]\n",
        "results_files_sorted = sorted(results_files, key=lambda x: int(x.partition(\"_\")[0]))\n",
        "\n",
        "# Initialize new df for aggregate of all subjects\n",
        "aggregated_df = pd.DataFrame()\n",
        "\n",
        "# Loop through all subjects\n",
        "for subject, file in enumerate(results_files_sorted):\n",
        "\n",
        "  # Read in results, rename trial column, and add subject_id and session columns\n",
        "  df_pci_filtered = pd.read_csv(input_output_measures_filtered + file)\n",
        "  df_pci_filtered.rename(columns = {'Unnamed: 0':'trial'}, inplace=True)\n",
        "  df_pci_filtered.rename(columns = {'bin':'condition'}, inplace=True)\n",
        "  df_pci_filtered['subject_id'] = subject\n",
        "  session = file[5]\n",
        "  df_pci_filtered['session'] = session\n",
        "  aggregated_df = pd.concat([aggregated_df, df_pci_filtered])\n",
        "\n",
        "# Reset a new sequential index\n",
        "aggregated_df = aggregated_df.reset_index(drop=True)\n",
        "\n",
        "# Save final aggregated csv for analysis in R\n",
        "aggregated_df.to_csv(input_output_measures_filtered + dataset + \"_chann\" + str(n_channels) +\n",
        "                     \"_base\" + str(baseline_start_factor) + \"_-\" + str(baseline_buffer) +\n",
        "                     \"_resp\" + str(response_end_factor) +\n",
        "                     \"_snr\" + str(snr_param) +\n",
        "                     \"_k\" + str(k_param) +\n",
        "                     \"_trial_ecode_aflags_condition_pci_lzc_subjectID_session_aggregated.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KHbF4rBBzQqp",
        "tjM41cZKzu-o",
        "M5lUOwvDP7_b",
        "c_rjvW00AeRQ",
        "l_r83pQJ56av",
        "_QhJ2DdJ2AYF",
        "mNDs0cS32Vjx",
        "moP3A9LyrMt9",
        "NWrLMBDTAsoY"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNNL1rcJzTRS4xTC95swbvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}